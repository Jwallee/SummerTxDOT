{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from /Users/grantrobinett/2024/coding/txdot_interpreted_fields/test_improvements/dataLocal/crash_data_1.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Read only the first 100,000 rows, avoiding encoding errors\n",
    "try:\n",
    "    raw_data_1 = pd.read_csv('./dataLocal/crash_data_1.csv', encoding='windows-1252', skiprows=0, nrows=100000, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error: Could not read the CSV file with encoding 'windows-1252'\")\n",
    "\n",
    "\n",
    "# List of columns to be predicted and therefore removed from the dataset\n",
    "columns_to_predict = [\n",
    "    \"Road_Relat_ID\", \"Intrsct_Relat_ID\", \"Road_Cls_ID\", \"Harm_Evnt_ID\",\n",
    "    \"FHE_Collsn_ID\", \"Obj_Struck_ID\", \"Phys_Featr_1_ID\", \"Phys_Featr_2_ID\",\n",
    "    \"Bridge_Detail_ID\", \"Othr_Factr_ID\", \"Road_Part_Adj_ID\", \"Investigator_Narrative\"\n",
    "]\n",
    "\n",
    "# Drop the columns to be predicted from the dataset\n",
    "data_features = raw_data_1.drop(columns=columns_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Checking the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the dataset have any columns from the list of columns to predict?\n",
      "False\n",
      "\n",
      "Size of the dataset:\n",
      "(100000, 162)\n",
      "\n",
      "Type of each column in the dataset:\n",
      "Crash_ID                   int64\n",
      "Crash_Fatal_Fl            object\n",
      "Cmv_Involv_Fl             object\n",
      "Schl_Bus_Fl               object\n",
      "Rr_Relat_Fl               object\n",
      "                          ...   \n",
      "Investigat_Service_ID    float64\n",
      "Investigat_DA_ID         float64\n",
      "Damage_1                  object\n",
      "Damage_2                  object\n",
      "Damage_3                  object\n",
      "Length: 162, dtype: object\n",
      "\n",
      "Unique types of columns in the dataset:\n",
      "[dtype('int64') dtype('O') dtype('float64')]\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset has any columns from the list of columns to predict, and print the result\n",
    "print(\"Does the dataset have any columns from the list of columns to predict?\")\n",
    "print(any(column in columns_to_predict for column in data_features.columns))\n",
    "\n",
    "# Print size of the dataset\n",
    "print(\"\\nSize of the dataset:\")\n",
    "print(data_features.shape)\n",
    "\n",
    "# Print type of each column in the dataset\n",
    "print(\"\\nType of each column in the dataset:\")\n",
    "print(data_features.dtypes)\n",
    "\n",
    "# Print types of columns in the dataset (unique types)\n",
    "print(\"\\nUnique types of columns in the dataset:\")\n",
    "print(data_features.dtypes.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More thorough look**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Rpt_Latitude', 'Rpt_Longitude', 'Rpt_Sec_Rdwy_Sys_ID',\n",
      "       'Rpt_Sec_Road_Part_ID', 'Rpt_Ref_Mark_Offset_Amt', 'Road_Type_ID',\n",
      "       'Investigat_Area_ID', 'Investigat_District_ID', 'Investigat_Region_ID',\n",
      "       'Latitude', 'Longitude', 'Dfo', 'Control', 'Section', 'Milepoint',\n",
      "       'Ref_Mark_Displ', 'Street_Nbr_2', 'Control_2', 'Section_2',\n",
      "       'Milepoint_2', 'Hwy_Dsgn_Lane_ID', 'Hwy_Dsgn_Hrt_ID', 'Hp_Shldr_Left',\n",
      "       'Hp_Shldr_Right', 'Hp_Median_Width', 'Base_Type_ID', 'Nbr_Of_Lane',\n",
      "       'Row_Width_Usual', 'Roadbed_Width', 'Surf_Width', 'Surf_Type_ID',\n",
      "       'Curb_Type_Left_ID', 'Curb_Type_Right_ID', 'Shldr_Type_Left_ID',\n",
      "       'Shldr_Width_Left', 'Shldr_Use_Left_ID', 'Shldr_Type_Right_ID',\n",
      "       'Shldr_Width_Right', 'Shldr_Use_Right_ID', 'Median_Type_ID',\n",
      "       'Median_Width', 'Rural_Urban_Type_ID', 'Func_Sys_ID', 'Adt_Curnt_Amt',\n",
      "       'Adt_Curnt_Year', 'Adt_Adj_Curnt_Amt', 'Pct_Single_Trk_Adt',\n",
      "       'Pct_Combo_Trk_Adt', 'Trk_Aadt_Pct', 'Curve_Type_ID', 'Curve_Lngth',\n",
      "       'Cd_Degr', 'Delta_Left_Right_ID', 'Dd_Degr', 'Feature_Crossed',\n",
      "       'Structure_Number', 'I_R_Min_Vert_Clear', 'Approach_Width',\n",
      "       'Bridge_Median_ID', 'Bridge_Loading_Type_ID',\n",
      "       'Bridge_Loading_In_1000_Lbs', 'Bridge_Srvc_Type_On_ID',\n",
      "       'Bridge_Srvc_Type_Under_ID', 'Culvert_Type_ID', 'Roadway_Width',\n",
      "       'Deck_Width', 'Bridge_Dir_Of_Traffic_ID', 'Bridge_Rte_Struct_Func_ID',\n",
      "       'Bridge_IR_Struct_Func_ID', 'RRCo', 'Poscrossing_ID', 'WDCode_ID',\n",
      "       'Standstop', 'Yield', 'MPO_ID', 'Investigat_Service_ID',\n",
      "       'Investigat_DA_ID'],\n",
      "      dtype='object')\n",
      "77\n",
      "Index(['Crash_ID', 'Rpt_CRIS_Cnty_ID', 'Rpt_City_ID', 'Rpt_Rdwy_Sys_ID',\n",
      "       'Rpt_Road_Part_ID', 'Crash_Speed_Limit', 'Wthr_Cond_ID',\n",
      "       'Light_Cond_ID', 'Entr_Road_ID', 'Road_Algn_ID', 'Surf_Cond_ID',\n",
      "       'Traffic_Cntl_ID', 'Investigat_Agency_ID', 'Cnty_ID', 'City_ID',\n",
      "       'Crash_Sev_ID', 'Pop_Group_ID', 'Sus_Serious_Injry_Cnt',\n",
      "       'Nonincap_Injry_Cnt', 'Poss_Injry_Cnt', 'Non_Injry_Cnt',\n",
      "       'Unkn_Injry_Cnt', 'Tot_Injry_Cnt', 'Death_Cnt'],\n",
      "      dtype='object')\n",
      "24\n",
      "Index(['Crash_Fatal_Fl', 'Cmv_Involv_Fl', 'Schl_Bus_Fl', 'Rr_Relat_Fl',\n",
      "       'Medical_Advisory_Fl', 'Amend_Supp_Fl', 'Active_School_Zone_Fl',\n",
      "       'Crash_Date', 'Crash_Time', 'Case_ID', 'Local_Use',\n",
      "       'Rpt_Outside_City_Limit_Fl', 'Thousand_Damage_Fl', 'Rpt_Hwy_Num',\n",
      "       'Rpt_Hwy_Sfx', 'Rpt_Block_Num', 'Rpt_Street_Pfx', 'Rpt_Street_Name',\n",
      "       'Rpt_Street_Sfx', 'Private_Dr_Fl', 'Toll_Road_Fl',\n",
      "       'Road_Constr_Zone_Fl', 'Road_Constr_Zone_Wrkr_Fl', 'Rpt_Street_Desc',\n",
      "       'At_Intrsct_Fl', 'Rpt_Sec_Hwy_Num', 'Rpt_Sec_Hwy_Sfx',\n",
      "       'Rpt_Sec_Block_Num', 'Rpt_Sec_Street_Pfx', 'Rpt_Sec_Street_Name',\n",
      "       'Rpt_Sec_Street_Sfx', 'Rpt_Ref_Mark_Dist_Uom', 'Rpt_Ref_Mark_Dir',\n",
      "       'Rpt_Ref_Mark_Nbr', 'Rpt_Sec_Street_Desc', 'Rpt_CrossingNumber',\n",
      "       'Investigat_Notify_Time', 'Investigat_Notify_Meth',\n",
      "       'Investigat_Arrv_Time', 'Report_Date', 'Investigat_Comp_Fl',\n",
      "       'ORI_Number', 'Hwy_Sys', 'Hwy_Nbr', 'Hwy_Sfx', 'Street_Name',\n",
      "       'Street_Nbr', 'Ref_Mark_Nbr', 'Hwy_Sys_2', 'Hwy_Nbr_2', 'Hwy_Sfx_2',\n",
      "       'Street_Name_2', 'Txdot_Rptable_Fl', 'Onsys_Fl', 'Rural_Fl',\n",
      "       'Located_Fl', 'Day_of_Week', 'CrossingNumber', 'Damage_1', 'Damage_2',\n",
      "       'Damage_3'],\n",
      "      dtype='object')\n",
      "61\n",
      "           Crash_ID  Rpt_CRIS_Cnty_ID    Rpt_City_ID  Rpt_Latitude  \\\n",
      "count  1.000000e+05     100000.000000  100000.000000  19337.000000   \n",
      "mean   1.873958e+07        108.311380    2183.152570     31.198400   \n",
      "std    4.799928e+04         69.443898    3888.597017      1.920604   \n",
      "min    1.867446e+07          1.000000       1.000000     25.881150   \n",
      "25%    1.870887e+07         57.000000     145.000000     29.827980   \n",
      "50%    1.873463e+07        101.000000     259.000000     31.710340   \n",
      "75%    1.875999e+07        161.000000     442.000000     32.733860   \n",
      "max    1.934116e+07        254.000000    9999.000000     36.415950   \n",
      "\n",
      "       Rpt_Longitude  Rpt_Rdwy_Sys_ID  Rpt_Road_Part_ID  Crash_Speed_Limit  \\\n",
      "count   19337.000000    100000.000000     100000.000000      100000.000000   \n",
      "mean      -97.560973        11.584440          1.377490          42.041840   \n",
      "std         2.232429         7.911686          1.257922          17.418315   \n",
      "min      -106.645920         1.000000          1.000000          -1.000000   \n",
      "25%       -98.428730         3.000000          1.000000          30.000000   \n",
      "50%       -97.319350        15.000000          1.000000          40.000000   \n",
      "75%       -95.732730        19.000000          1.000000          55.000000   \n",
      "max       -93.507950        19.000000          7.000000          85.000000   \n",
      "\n",
      "       Rpt_Sec_Rdwy_Sys_ID  Rpt_Sec_Road_Part_ID  ...  Sus_Serious_Injry_Cnt  \\\n",
      "count         95930.000000          95930.000000  ...           100000.00000   \n",
      "mean             15.948577              1.189253  ...                0.02644   \n",
      "std               5.904722              0.832443  ...                0.18863   \n",
      "min               1.000000              1.000000  ...                0.00000   \n",
      "25%              15.000000              1.000000  ...                0.00000   \n",
      "50%              19.000000              1.000000  ...                0.00000   \n",
      "75%              19.000000              1.000000  ...                0.00000   \n",
      "max              19.000000              7.000000  ...                6.00000   \n",
      "\n",
      "       Nonincap_Injry_Cnt  Poss_Injry_Cnt  Non_Injry_Cnt  Unkn_Injry_Cnt  \\\n",
      "count       100000.000000   100000.000000  100000.000000   100000.000000   \n",
      "mean             0.130910        0.207830       1.907570        0.200560   \n",
      "std              0.440675        0.580431       1.809889        0.490467   \n",
      "min              0.000000        0.000000       0.000000        0.000000   \n",
      "25%              0.000000        0.000000       1.000000        0.000000   \n",
      "50%              0.000000        0.000000       2.000000        0.000000   \n",
      "75%              0.000000        0.000000       2.000000        0.000000   \n",
      "max             13.000000       15.000000      74.000000       36.000000   \n",
      "\n",
      "       Tot_Injry_Cnt      Death_Cnt        MPO_ID  Investigat_Service_ID  \\\n",
      "count  100000.000000  100000.000000  81357.000000           94310.000000   \n",
      "mean        0.365180       0.006260    130.011579             105.316191   \n",
      "std         0.754632       0.085328    113.883672             186.618964   \n",
      "min         0.000000       0.000000     15.000000               1.000000   \n",
      "25%         0.000000       0.000000     15.000000              28.000000   \n",
      "50%         0.000000       0.000000     90.000000              35.000000   \n",
      "75%         1.000000       0.000000    282.000000             123.000000   \n",
      "max        16.000000       3.000000    363.000000            1468.000000   \n",
      "\n",
      "       Investigat_DA_ID  \n",
      "count      34606.000000  \n",
      "mean         109.546293  \n",
      "std          265.725865  \n",
      "min            1.000000  \n",
      "25%            8.000000  \n",
      "50%           17.000000  \n",
      "75%           63.000000  \n",
      "max         1567.000000  \n",
      "\n",
      "[8 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print all float column names in the dataset\n",
    "print(data_features.select_dtypes(include=['float64']).columns)\n",
    "# Print number of float columns in the dataset\n",
    "print(len(data_features.select_dtypes(include=['float64']).columns))\n",
    "\n",
    "# Print all integer column names in the dataset\n",
    "print(data_features.select_dtypes(include=['int64']).columns)\n",
    "# Print number of integer columns in the dataset\n",
    "print(len(data_features.select_dtypes(include=['int64']).columns))\n",
    "\n",
    "# Print all object column names in the dataset\n",
    "print(data_features.select_dtypes(include=['object']).columns)\n",
    "# Print number of object columns in the dataset\n",
    "print(len(data_features.select_dtypes(include=['object']).columns))\n",
    "\n",
    "# Print summary statistics of the dataset\n",
    "print(data_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *At least the values come in pretty good formats, I do not see any columns that have incorrect types at first glance. Let's check for missing data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in the dataset:\n",
      "Crash_ID                     0\n",
      "Crash_Fatal_Fl               0\n",
      "Cmv_Involv_Fl                0\n",
      "Schl_Bus_Fl                  0\n",
      "Rr_Relat_Fl                  0\n",
      "                         ...  \n",
      "Investigat_Service_ID     5690\n",
      "Investigat_DA_ID         65394\n",
      "Damage_1                 85968\n",
      "Damage_2                 98391\n",
      "Damage_3                 99747\n",
      "Length: 162, dtype: int64\n",
      "\n",
      "Columns with missing values in the dataset:\n",
      "Index(['Case_ID', 'Local_Use', 'Rpt_Latitude', 'Rpt_Longitude', 'Rpt_Hwy_Num',\n",
      "       'Rpt_Hwy_Sfx', 'Rpt_Block_Num', 'Rpt_Street_Pfx', 'Rpt_Street_Name',\n",
      "       'Rpt_Street_Sfx',\n",
      "       ...\n",
      "       'Poscrossing_ID', 'WDCode_ID', 'Standstop', 'Yield', 'MPO_ID',\n",
      "       'Investigat_Service_ID', 'Investigat_DA_ID', 'Damage_1', 'Damage_2',\n",
      "       'Damage_3'],\n",
      "      dtype='object', length=113)\n",
      "\n",
      "Number of columns with missing values in the dataset:\n",
      "113\n",
      "\n",
      "Columns with missing values and their missing value count:\n",
      "Case_ID                         7670\n",
      "Local_Use                      68629\n",
      "Rpt_Latitude                   80663\n",
      "Rpt_Longitude                  80663\n",
      "Rpt_Hwy_Num                    54016\n",
      "Rpt_Hwy_Sfx                    96782\n",
      "Rpt_Block_Num                  13345\n",
      "Rpt_Street_Pfx                 61982\n",
      "Rpt_Street_Name                    3\n",
      "Rpt_Street_Sfx                 27534\n",
      "Rpt_Street_Desc                69995\n",
      "Rpt_Sec_Rdwy_Sys_ID             4070\n",
      "Rpt_Sec_Hwy_Num                80759\n",
      "Rpt_Sec_Hwy_Sfx                98906\n",
      "Rpt_Sec_Road_Part_ID            4070\n",
      "Rpt_Sec_Block_Num              22958\n",
      "Rpt_Sec_Street_Pfx             74725\n",
      "Rpt_Sec_Street_Name                3\n",
      "Rpt_Sec_Street_Sfx             22327\n",
      "Rpt_Ref_Mark_Offset_Amt        95930\n",
      "Rpt_Ref_Mark_Dist_Uom          95930\n",
      "Rpt_Ref_Mark_Dir               95944\n",
      "Rpt_Ref_Mark_Nbr               95930\n",
      "Rpt_Sec_Street_Desc            80024\n",
      "Rpt_CrossingNumber             99948\n",
      "Road_Type_ID                   51207\n",
      "Investigat_Notify_Meth             4\n",
      "ORI_Number                      9906\n",
      "Investigat_Area_ID            100000\n",
      "Investigat_District_ID        100000\n",
      "Investigat_Region_ID           52525\n",
      "Latitude                        8634\n",
      "Longitude                       8634\n",
      "Hwy_Sys                        48618\n",
      "Hwy_Nbr                        48618\n",
      "Hwy_Sfx                        96214\n",
      "Dfo                            50868\n",
      "Street_Name                        3\n",
      "Street_Nbr                     57561\n",
      "Control                        51207\n",
      "Section                        51207\n",
      "Milepoint                      51207\n",
      "Ref_Mark_Nbr                   51342\n",
      "Ref_Mark_Displ                 51342\n",
      "Hwy_Sys_2                      91769\n",
      "Hwy_Nbr_2                      91769\n",
      "Hwy_Sfx_2                      99473\n",
      "Street_Name_2                  59191\n",
      "Street_Nbr_2                  100000\n",
      "Control_2                      97321\n",
      "Section_2                      97321\n",
      "Milepoint_2                    97321\n",
      "Hwy_Dsgn_Lane_ID               51824\n",
      "Hwy_Dsgn_Hrt_ID               100000\n",
      "Hp_Shldr_Left                  51824\n",
      "Hp_Shldr_Right                 51824\n",
      "Hp_Median_Width                51824\n",
      "Base_Type_ID                   98699\n",
      "Nbr_Of_Lane                    51755\n",
      "Row_Width_Usual                51824\n",
      "Roadbed_Width                  51824\n",
      "Surf_Width                     51824\n",
      "Surf_Type_ID                   51824\n",
      "Curb_Type_Left_ID              52178\n",
      "Curb_Type_Right_ID             52418\n",
      "Shldr_Type_Left_ID             51824\n",
      "Shldr_Width_Left               51824\n",
      "Shldr_Use_Left_ID              51824\n",
      "Shldr_Type_Right_ID            51824\n",
      "Shldr_Width_Right              51824\n",
      "Shldr_Use_Right_ID             51824\n",
      "Median_Type_ID                 51824\n",
      "Median_Width                   51824\n",
      "Rural_Urban_Type_ID            51207\n",
      "Func_Sys_ID                    51824\n",
      "Adt_Curnt_Amt                  51965\n",
      "Adt_Curnt_Year                 51965\n",
      "Adt_Adj_Curnt_Amt              51965\n",
      "Pct_Single_Trk_Adt             51965\n",
      "Pct_Combo_Trk_Adt              51965\n",
      "Trk_Aadt_Pct                   51965\n",
      "Curve_Type_ID                 100000\n",
      "Curve_Lngth                   100000\n",
      "Cd_Degr                       100000\n",
      "Delta_Left_Right_ID           100000\n",
      "Dd_Degr                       100000\n",
      "Feature_Crossed               100000\n",
      "Structure_Number              100000\n",
      "I_R_Min_Vert_Clear            100000\n",
      "Approach_Width                100000\n",
      "Bridge_Median_ID              100000\n",
      "Bridge_Loading_Type_ID        100000\n",
      "Bridge_Loading_In_1000_Lbs    100000\n",
      "Bridge_Srvc_Type_On_ID        100000\n",
      "Bridge_Srvc_Type_Under_ID     100000\n",
      "Culvert_Type_ID               100000\n",
      "Roadway_Width                 100000\n",
      "Deck_Width                    100000\n",
      "Bridge_Dir_Of_Traffic_ID      100000\n",
      "Bridge_Rte_Struct_Func_ID     100000\n",
      "Bridge_IR_Struct_Func_ID      100000\n",
      "CrossingNumber                 99948\n",
      "RRCo                          100000\n",
      "Poscrossing_ID                100000\n",
      "WDCode_ID                     100000\n",
      "Standstop                     100000\n",
      "Yield                         100000\n",
      "MPO_ID                         18643\n",
      "Investigat_Service_ID           5690\n",
      "Investigat_DA_ID               65394\n",
      "Damage_1                       85968\n",
      "Damage_2                       98391\n",
      "Damage_3                       99747\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows in the dataset:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(data_features.isnull().sum())\n",
    "\n",
    "# Print columns with missing values in the dataset\n",
    "print(\"\\nColumns with missing values in the dataset:\")\n",
    "print(data_features.columns[data_features.isnull().any()])\n",
    "\n",
    "# Print number of columns with missing values in the dataset\n",
    "print(\"\\nNumber of columns with missing values in the dataset:\")\n",
    "print(data_features.columns[data_features.isnull().any()].size)\n",
    "\n",
    "# Print columns with missing values, with format \"column_name: missing_value_count\". Avoid elipses when printing all columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(\"\\nColumns with missing values and their missing value count:\")\n",
    "print(data_features.isnull().sum()[data_features.isnull().sum() > 0])\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "\n",
    "# Check for duplicate rows in the dataset\n",
    "print(\"\\nNumber of duplicate rows in the dataset:\")\n",
    "print(data_features.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *As we can see, there are some columns with missing values for every row loaded into the dataset. I'm going to make the executive decision to drop these*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the dataset after dropping columns with missing values:\n",
      "(100000, 49)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with any missing values\n",
    "data_features_no_missing = data_features.dropna(axis=1)\n",
    "\n",
    "# Print size of the dataset after dropping columns with missing values\n",
    "print(\"\\nSize of the dataset after dropping columns with missing values:\")\n",
    "print(data_features_no_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the dataset after dropping columns with all missing values:\n",
      "(100000, 133)\n",
      "\n",
      "Missing values in the new dataset:\n",
      "Crash_ID                     0\n",
      "Crash_Fatal_Fl               0\n",
      "Cmv_Involv_Fl                0\n",
      "Schl_Bus_Fl                  0\n",
      "Rr_Relat_Fl                  0\n",
      "                         ...  \n",
      "Investigat_Service_ID     5690\n",
      "Investigat_DA_ID         65394\n",
      "Damage_1                 85968\n",
      "Damage_2                 98391\n",
      "Damage_3                 99747\n",
      "Length: 133, dtype: int64\n",
      "\n",
      "Columns with missing values in the new dataset:\n",
      "Index(['Case_ID', 'Local_Use', 'Rpt_Latitude', 'Rpt_Longitude', 'Rpt_Hwy_Num',\n",
      "       'Rpt_Hwy_Sfx', 'Rpt_Block_Num', 'Rpt_Street_Pfx', 'Rpt_Street_Name',\n",
      "       'Rpt_Street_Sfx', 'Rpt_Street_Desc', 'Rpt_Sec_Rdwy_Sys_ID',\n",
      "       'Rpt_Sec_Hwy_Num', 'Rpt_Sec_Hwy_Sfx', 'Rpt_Sec_Road_Part_ID',\n",
      "       'Rpt_Sec_Block_Num', 'Rpt_Sec_Street_Pfx', 'Rpt_Sec_Street_Name',\n",
      "       'Rpt_Sec_Street_Sfx', 'Rpt_Ref_Mark_Offset_Amt',\n",
      "       'Rpt_Ref_Mark_Dist_Uom', 'Rpt_Ref_Mark_Dir', 'Rpt_Ref_Mark_Nbr',\n",
      "       'Rpt_Sec_Street_Desc', 'Rpt_CrossingNumber', 'Road_Type_ID',\n",
      "       'Investigat_Notify_Meth', 'ORI_Number', 'Investigat_Region_ID',\n",
      "       'Latitude', 'Longitude', 'Hwy_Sys', 'Hwy_Nbr', 'Hwy_Sfx', 'Dfo',\n",
      "       'Street_Name', 'Street_Nbr', 'Control', 'Section', 'Milepoint',\n",
      "       'Ref_Mark_Nbr', 'Ref_Mark_Displ', 'Hwy_Sys_2', 'Hwy_Nbr_2', 'Hwy_Sfx_2',\n",
      "       'Street_Name_2', 'Control_2', 'Section_2', 'Milepoint_2',\n",
      "       'Hwy_Dsgn_Lane_ID', 'Hp_Shldr_Left', 'Hp_Shldr_Right',\n",
      "       'Hp_Median_Width', 'Base_Type_ID', 'Nbr_Of_Lane', 'Row_Width_Usual',\n",
      "       'Roadbed_Width', 'Surf_Width', 'Surf_Type_ID', 'Curb_Type_Left_ID',\n",
      "       'Curb_Type_Right_ID', 'Shldr_Type_Left_ID', 'Shldr_Width_Left',\n",
      "       'Shldr_Use_Left_ID', 'Shldr_Type_Right_ID', 'Shldr_Width_Right',\n",
      "       'Shldr_Use_Right_ID', 'Median_Type_ID', 'Median_Width',\n",
      "       'Rural_Urban_Type_ID', 'Func_Sys_ID', 'Adt_Curnt_Amt', 'Adt_Curnt_Year',\n",
      "       'Adt_Adj_Curnt_Amt', 'Pct_Single_Trk_Adt', 'Pct_Combo_Trk_Adt',\n",
      "       'Trk_Aadt_Pct', 'CrossingNumber', 'MPO_ID', 'Investigat_Service_ID',\n",
      "       'Investigat_DA_ID', 'Damage_1', 'Damage_2', 'Damage_3'],\n",
      "      dtype='object')\n",
      "\n",
      "Number of columns with missing values in the new dataset:\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# # # Drop columns with missing values = 100,000. Only drop columns with exactly 100,000 missing values. Save the result to a new dataset\n",
    "# data_features_dropped = data_features.dropna(axis=1, how='all')\n",
    "\n",
    "# # Print size of the new dataset after dropping columns with missing values = 100,000\n",
    "# print(\"\\nSize of the dataset after dropping columns with all missing values:\")\n",
    "# print(data_features_dropped.shape)\n",
    "\n",
    "# # Check for missing values in the new dataset\n",
    "# print(\"\\nMissing values in the new dataset:\")\n",
    "# print(data_features_dropped.isnull().sum())\n",
    "\n",
    "# # Print columns with missing values in the new dataset\n",
    "# print(\"\\nColumns with missing values in the new dataset:\")\n",
    "# print(data_features_dropped.columns[data_features_dropped.isnull().any()])\n",
    "# # Print number of columns with missing values in the new dataset\n",
    "# print(\"\\nNumber of columns with missing values in the new dataset:\")\n",
    "# print(data_features_dropped.columns[data_features_dropped.isnull().any()].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *With the worst NA columns gone, we can go to processing numerical and categorical data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics of the normalized dataset:\n",
      "            Crash_ID  Rpt_CRIS_Cnty_ID    Rpt_City_ID  Rpt_Rdwy_Sys_ID  \\\n",
      "count  100000.000000     100000.000000  100000.000000    100000.000000   \n",
      "mean        0.097667          0.424156       0.218259         0.588024   \n",
      "std         0.071995          0.274482       0.388937         0.439538   \n",
      "min         0.000000          0.000000       0.000000         0.000000   \n",
      "25%         0.051605          0.221344       0.014403         0.111111   \n",
      "50%         0.090245          0.395257       0.025805         0.777778   \n",
      "75%         0.128279          0.632411       0.044109         1.000000   \n",
      "max         1.000000          1.000000       1.000000         1.000000   \n",
      "\n",
      "       Rpt_Road_Part_ID  Crash_Speed_Limit   Wthr_Cond_ID  Light_Cond_ID  \\\n",
      "count     100000.000000      100000.000000  100000.000000  100000.000000   \n",
      "mean           0.062915           0.500487       0.848073       0.250896   \n",
      "std            0.209654           0.202539       0.232913       0.176689   \n",
      "min            0.000000           0.000000       0.000000       0.000000   \n",
      "25%            0.000000           0.360465       0.916667       0.125000   \n",
      "50%            0.000000           0.476744       0.916667       0.125000   \n",
      "75%            0.000000           0.651163       0.916667       0.500000   \n",
      "max            1.000000           1.000000       1.000000       1.000000   \n",
      "\n",
      "        Entr_Road_ID   Road_Algn_ID  ...        City_ID   Crash_Sev_ID  \\\n",
      "count  100000.000000  100000.000000  ...  100000.000000  100000.000000   \n",
      "mean        0.122432       0.066369  ...       0.234213       0.807502   \n",
      "std         0.308553       0.170690  ...       0.401257       0.308392   \n",
      "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
      "25%         0.000000       0.000000  ...       0.015003       0.600000   \n",
      "50%         0.020408       0.000000  ...       0.026305       1.000000   \n",
      "75%         0.040816       0.000000  ...       0.044709       1.000000   \n",
      "max         1.000000       1.000000  ...       1.000000       1.000000   \n",
      "\n",
      "        Pop_Group_ID  Sus_Serious_Injry_Cnt  Nonincap_Injry_Cnt  \\\n",
      "count  100000.000000          100000.000000       100000.000000   \n",
      "mean        0.660770               0.004407            0.010070   \n",
      "std         0.393089               0.031438            0.033898   \n",
      "min         0.000000               0.000000            0.000000   \n",
      "25%         0.444444               0.000000            0.000000   \n",
      "50%         0.888889               0.000000            0.000000   \n",
      "75%         1.000000               0.000000            0.000000   \n",
      "max         1.000000               1.000000            1.000000   \n",
      "\n",
      "       Poss_Injry_Cnt  Non_Injry_Cnt  Unkn_Injry_Cnt  Tot_Injry_Cnt  \\\n",
      "count   100000.000000  100000.000000   100000.000000  100000.000000   \n",
      "mean         0.013855       0.025778        0.005571       0.022824   \n",
      "std          0.038695       0.024458        0.013624       0.047164   \n",
      "min          0.000000       0.000000        0.000000       0.000000   \n",
      "25%          0.000000       0.013514        0.000000       0.000000   \n",
      "50%          0.000000       0.027027        0.000000       0.000000   \n",
      "75%          0.000000       0.027027        0.000000       0.062500   \n",
      "max          1.000000       1.000000        1.000000       1.000000   \n",
      "\n",
      "           Death_Cnt  \n",
      "count  100000.000000  \n",
      "mean        0.002087  \n",
      "std         0.028443  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Normalize numerical columns in the dataset to be between 0 and 1\n",
    "data_features_normalized = data_features_no_missing.copy()\n",
    "numerical_columns = data_features_normalized.select_dtypes(include=['float64', 'int64']).columns\n",
    "data_features_normalized[numerical_columns] = (data_features_normalized[numerical_columns] - data_features_normalized[numerical_columns].min()) / (data_features_normalized[numerical_columns].max() - data_features_normalized[numerical_columns].min())\n",
    "\n",
    "# Print summary statistics of the normalized dataset\n",
    "print(\"\\nSummary statistics of the normalized dataset:\")\n",
    "print(data_features_normalized.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the dataset before one-hot encoding:\n",
      "(100000, 49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the dataset after one-hot encoding:\n",
      "(100000, 4981)\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical columns in the dataset using one-hot encoding\n",
    "# Print size of the dataset before one-hot encoding\n",
    "print(\"\\nSize of the dataset before one-hot encoding:\")\n",
    "print(data_features_normalized.shape)\n",
    "data_features_encoded = pd.get_dummies(data_features_normalized)   # One-hot encoding\n",
    "# Print size of the dataset after one-hot encoding\n",
    "print(\"\\nSize of the dataset after one-hot encoding:\")\n",
    "print(data_features_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *For now, basic normalization and basic one-hot encoding has beed done. Let's see how models do.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the dataset:\n",
      "(100000, 4981)\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the dataset\n",
    "print(\"\\nSize of the dataset:\")\n",
    "print(data_features_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:45:21.408022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# To train our first model, we will be predicting the column \"Road_Cls_ID\"\n",
    "# We will split the dataset into features (X) and target (y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming `raw_data_1` is your original dataframe and contains the \"Road_Cls_ID\" column.\n",
    "\n",
    "# Encode the categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "# Adjust labels to zero-based if originally one-based\n",
    "adjusted_labels = raw_data_1[\"Road_Cls_ID\"] - 1\n",
    "encoded_target = label_encoder.fit_transform(adjusted_labels)\n",
    "encoded_target = to_categorical(encoded_target)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features_encoded,  # this is the data after one-hot encoding the features\n",
    "    encoded_target,  # this is your one-hot encoded target variable\n",
    "    test_size=0.2,   # usually, 20% of the data is used for testing\n",
    "    stratify=encoded_target,  # this is used to ensure that the distribution of classes in the train and test sets are similar\n",
    "    random_state=42  # seed for reproducibility of results\n",
    ")\n",
    "\n",
    "# Now you can use `X_train`, `y_train` for training the model and `X_test`, `y_test` for evaluating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 44s 19ms/step - loss: 0.7536 - accuracy: 0.7122 - val_loss: 0.5491 - val_accuracy: 0.7995\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 42s 21ms/step - loss: 0.5188 - accuracy: 0.8118 - val_loss: 0.4499 - val_accuracy: 0.8486\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.3917 - accuracy: 0.8708 - val_loss: 0.3699 - val_accuracy: 0.8851\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.2898 - accuracy: 0.9126 - val_loss: 0.3889 - val_accuracy: 0.8903\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.2310 - accuracy: 0.9325 - val_loss: 0.3967 - val_accuracy: 0.8956\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 35s 18ms/step - loss: 0.1833 - accuracy: 0.9453 - val_loss: 0.5540 - val_accuracy: 0.8788\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 40s 20ms/step - loss: 0.1528 - accuracy: 0.9536 - val_loss: 0.4957 - val_accuracy: 0.8942\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 40s 20ms/step - loss: 0.1317 - accuracy: 0.9583 - val_loss: 0.6148 - val_accuracy: 0.8966\n",
      "\n",
      "Model evaluation:\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3878 - accuracy: 0.8810\n",
      "Loss: 0.38776895403862\n",
      "Accuracy: 0.8810499906539917\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))  # input layer and first hidden layer\n",
    "model.add(Dense(128, activation='relu'))  # second hidden layer\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "model.add(Dense(64, activation='relu'))  # third hidden layer\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "model.add(Dense(32, activation='relu'))  # fourth hidden layer\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50,  # Increased epochs due to early stopping\n",
    "                    batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Print model metrics\n",
    "print(\"\\nModel evaluation:\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/ANN_v2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/ANN_v2/assets\n"
     ]
    }
   ],
   "source": [
    "# # Save the model\n",
    "# model.save('models/ANN_model_v1.keras')\n",
    "\n",
    "# Saving as tf\n",
    "model.save('models/ANN_v2', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 35s 16ms/step - loss: 2.2667 - accuracy: 0.6597 - val_loss: 0.9520 - val_accuracy: 0.7839\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 30s 15ms/step - loss: 1.0392 - accuracy: 0.7333 - val_loss: 0.8473 - val_accuracy: 0.7806\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 0.9702 - accuracy: 0.7382 - val_loss: 0.8628 - val_accuracy: 0.7433\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 0.9433 - accuracy: 0.7363 - val_loss: 0.7929 - val_accuracy: 0.7722\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 31s 16ms/step - loss: 0.9254 - accuracy: 0.7355 - val_loss: 0.7742 - val_accuracy: 0.7918\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.9089 - accuracy: 0.7393 - val_loss: 0.8110 - val_accuracy: 0.7811\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 33s 16ms/step - loss: 0.9031 - accuracy: 0.7388 - val_loss: 0.7816 - val_accuracy: 0.7830\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8897 - accuracy: 0.7441 - val_loss: 0.7526 - val_accuracy: 0.8020\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8902 - accuracy: 0.7437 - val_loss: 0.7663 - val_accuracy: 0.7618\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8841 - accuracy: 0.7439 - val_loss: 0.7310 - val_accuracy: 0.8004\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8940 - accuracy: 0.7405 - val_loss: 0.7427 - val_accuracy: 0.7893\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8816 - accuracy: 0.7440 - val_loss: 0.7239 - val_accuracy: 0.7964\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8778 - accuracy: 0.7431 - val_loss: 0.7374 - val_accuracy: 0.7846\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8769 - accuracy: 0.7451 - val_loss: 0.7377 - val_accuracy: 0.7782\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8836 - accuracy: 0.7416 - val_loss: 0.7836 - val_accuracy: 0.7504\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8763 - accuracy: 0.7441 - val_loss: 0.7519 - val_accuracy: 0.7773\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 32s 16ms/step - loss: 0.8759 - accuracy: 0.7458 - val_loss: 0.7931 - val_accuracy: 0.7461\n",
      "\n",
      "Model evaluation:\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.7415 - accuracy: 0.7882\n",
      "Loss: 0.7414984107017517\n",
      "Accuracy: 0.7882000207901001\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
    "model.add(BatchNormalization())  # Batch normalization\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "model.add(Dense(128, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
    "model.add(BatchNormalization())  # Batch normalization\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "model.add(Dense(64, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
    "model.add(BatchNormalization())  # Batch normalization\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "model.add(Dense(32, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))  # L2 regularization\n",
    "model.add(BatchNormalization())  # Batch normalization\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50,  # Increased epochs due to early stopping\n",
    "                    batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Print model metrics\n",
    "print(\"\\nModel evaluation:\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('models/ANN_model_v2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 05m 29s]\n",
      "val_accuracy: 0.9089375138282776\n",
      "\n",
      "Best val_accuracy So Far: 0.9100000262260437\n",
      "Total elapsed time: 01h 11m 35s\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 29s 14ms/step - loss: 0.7225 - accuracy: 0.7336 - val_loss: 0.5451 - val_accuracy: 0.7883\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.4603 - accuracy: 0.8346 - val_loss: 0.4303 - val_accuracy: 0.8469\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 0.3613 - accuracy: 0.8839 - val_loss: 0.3767 - val_accuracy: 0.8802\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.2934 - accuracy: 0.9155 - val_loss: 0.3487 - val_accuracy: 0.8954\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.2476 - accuracy: 0.9323 - val_loss: 0.3256 - val_accuracy: 0.9087\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.2198 - accuracy: 0.9392 - val_loss: 0.3971 - val_accuracy: 0.8812\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.1927 - accuracy: 0.9464 - val_loss: 0.3499 - val_accuracy: 0.9107\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.1711 - accuracy: 0.9513 - val_loss: 0.3975 - val_accuracy: 0.8903\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.1476 - accuracy: 0.9567 - val_loss: 0.3879 - val_accuracy: 0.9094\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.1252 - accuracy: 0.9626 - val_loss: 0.4176 - val_accuracy: 0.9038\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.1034 - accuracy: 0.9687 - val_loss: 0.5767 - val_accuracy: 0.8706\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0833 - accuracy: 0.9743 - val_loss: 0.5050 - val_accuracy: 0.9034\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 0.0639 - accuracy: 0.9809 - val_loss: 0.5768 - val_accuracy: 0.8924\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0518 - accuracy: 0.9843 - val_loss: 0.6013 - val_accuracy: 0.8953\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0403 - accuracy: 0.9880 - val_loss: 0.7133 - val_accuracy: 0.8587\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0294 - accuracy: 0.9915 - val_loss: 0.7325 - val_accuracy: 0.8813\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0279 - accuracy: 0.9915 - val_loss: 0.8551 - val_accuracy: 0.8683\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.8888 - val_accuracy: 0.8859\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.8634 - val_accuracy: 0.8966\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0151 - accuracy: 0.9956 - val_loss: 0.9115 - val_accuracy: 0.8778\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 0.8959 - val_accuracy: 0.8849\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.9620 - val_accuracy: 0.8862\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.9850 - val_accuracy: 0.8891\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.9542 - val_accuracy: 0.8918\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.9563 - val_accuracy: 0.8893\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0059 - accuracy: 0.9983 - val_loss: 0.9931 - val_accuracy: 0.8834\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 1.0270 - val_accuracy: 0.8970\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 1.0764 - val_accuracy: 0.8719\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 1.0096 - val_accuracy: 0.8921\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0070 - accuracy: 0.9978 - val_loss: 1.1873 - val_accuracy: 0.8434\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 1.0354 - val_accuracy: 0.8894\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 1.0561 - val_accuracy: 0.8966\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 1.2846 - val_accuracy: 0.8519\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 1.1201 - val_accuracy: 0.8768\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 28s 14ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 1.0846 - val_accuracy: 0.8917\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 1.0319 - val_accuracy: 0.8898\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 1.0512 - val_accuracy: 0.8941\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 1.0555 - val_accuracy: 0.8936\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 1.1195 - val_accuracy: 0.8796\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 26s 13ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 1.0533 - val_accuracy: 0.8867\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 1.1733 - val_accuracy: 0.8898\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 1.2148 - val_accuracy: 0.8629\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 1.0438 - val_accuracy: 0.8975\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 1.0618 - val_accuracy: 0.8804\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 1.0947 - val_accuracy: 0.8907\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 27s 14ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 1.0509 - val_accuracy: 0.8901\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 1.0666 - val_accuracy: 0.8967\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 1.0978 - val_accuracy: 0.8797\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 1.0463 - val_accuracy: 0.8904\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 27s 13ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 1.0822 - val_accuracy: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1400f84c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(hp.Int('input_units', min_value=32, max_value=256, step=32),\n",
    "                           activation='relu', input_dim=X_train.shape[1]))\n",
    "    \n",
    "    for i in range(hp.Int('n_layers', 1, 4)):  # Number of hidden layers\n",
    "        model.add(layers.Dense(hp.Int(f'dense_{i}_units', min_value=32, max_value=256, step=32),\n",
    "                               activation='relu'))\n",
    "        model.add(layers.Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, default=0.25, step=0.05)))\n",
    "    \n",
    "    model.add(layers.Dense(y_train.shape[1], activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,  # The number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # The number of models that should be built and fit for each trial\n",
    "    directory='my_dir',  # Directory to store tuning logs\n",
    "    project_name='keras_tuning')\n",
    "\n",
    "# Display search space overview\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Perform the hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the best hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model to a file\n",
    "best_model.save('tuning_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 25 variables whereas the saved optimizer has 1 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural network summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 224)               1115968   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                7200      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 192)               6336      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               24704     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9)                 2313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1189545 (4.54 MB)\n",
      "Trainable params: 1189545 (4.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Assets written to: models/tuned/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/tuned/assets\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best model from a file\n",
    "loaded_model = keras.models.load_model('models/tuning_1.keras')\n",
    "\n",
    "# #print metrics\n",
    "# print(\"\\nModel evaluation:\")\n",
    "# loss, accuracy = loaded_model.evaluate(X_test, y_test)\n",
    "# print(f\"Loss: {loss}\")\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print neural network summary\n",
    "print(\"\\nNeural network summary:\")\n",
    "print(loaded_model.summary())\n",
    "\n",
    "loaded_model.save('models/tuned', save_format='tf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size of the dataset after dropping columns with missing values:\n",
      "(100, 55)\n",
      "\n",
      "Summary statistics of the normalized dataset:\n",
      "         Crash_ID  Rpt_CRIS_Cnty_ID  Rpt_City_ID  Rpt_Rdwy_Sys_ID  \\\n",
      "count  100.000000        100.000000   100.000000       100.000000   \n",
      "mean     0.152728          0.493420     0.088460         0.642222   \n",
      "std      0.087821          0.399352     0.252361         0.439317   \n",
      "min      0.000000          0.000000     0.000000         0.000000   \n",
      "25%      0.146319          0.000000     0.000000         0.152778   \n",
      "50%      0.146576          0.372294     0.019044         1.000000   \n",
      "75%      0.146775          0.917749     0.035782         1.000000   \n",
      "max      1.000000          1.000000     1.000000         1.000000   \n",
      "\n",
      "       Rpt_Road_Part_ID  Crash_Speed_Limit  Rpt_Sec_Rdwy_Sys_ID  \\\n",
      "count        100.000000         100.000000           100.000000   \n",
      "mean           0.040000           0.558553             0.913333   \n",
      "std            0.130009           0.244825             0.259658   \n",
      "min            0.000000           0.000000             0.000000   \n",
      "25%            0.000000           0.473684             1.000000   \n",
      "50%            0.000000           0.605263             1.000000   \n",
      "75%            0.000000           0.736842             1.000000   \n",
      "max            1.000000           1.000000             1.000000   \n",
      "\n",
      "       Rpt_Sec_Road_Part_ID  Wthr_Cond_ID  Light_Cond_ID  ...     City_ID  \\\n",
      "count            100.000000    100.000000     100.000000  ...  100.000000   \n",
      "mean               0.041667      0.897000       0.338333  ...    0.105626   \n",
      "std                0.161320      0.133678       0.244416  ...    0.283124   \n",
      "min                0.000000      0.000000       0.000000  ...    0.000000   \n",
      "25%                0.000000      0.900000       0.166667  ...    0.000000   \n",
      "50%                0.000000      0.900000       0.166667  ...    0.018643   \n",
      "75%                0.000000      0.900000       0.666667  ...    0.035782   \n",
      "max                1.000000      1.000000       1.000000  ...    1.000000   \n",
      "\n",
      "       Crash_Sev_ID  Pop_Group_ID  Sus_Serious_Injry_Cnt  Nonincap_Injry_Cnt  \\\n",
      "count    100.000000    100.000000             100.000000          100.000000   \n",
      "mean       0.804000      0.858889               0.015000            0.056667   \n",
      "std        0.298115      0.301658               0.111351            0.157527   \n",
      "min        0.000000      0.000000               0.000000            0.000000   \n",
      "25%        0.600000      0.888889               0.000000            0.000000   \n",
      "50%        1.000000      1.000000               0.000000            0.000000   \n",
      "75%        1.000000      1.000000               0.000000            0.000000   \n",
      "max        1.000000      1.000000               1.000000            1.000000   \n",
      "\n",
      "       Poss_Injry_Cnt  Non_Injry_Cnt  Unkn_Injry_Cnt  Tot_Injry_Cnt  \\\n",
      "count      100.000000     100.000000      100.000000      100.00000   \n",
      "mean         0.120000       0.400000        0.060000        0.11000   \n",
      "std          0.285332       0.275608        0.159826        0.20201   \n",
      "min          0.000000       0.000000        0.000000        0.00000   \n",
      "25%          0.000000       0.200000        0.000000        0.00000   \n",
      "50%          0.000000       0.400000        0.000000        0.00000   \n",
      "75%          0.000000       0.600000        0.000000        0.25000   \n",
      "max          1.000000       1.000000        1.000000        1.00000   \n",
      "\n",
      "        Death_Cnt  \n",
      "count  100.000000  \n",
      "mean     0.020000  \n",
      "std      0.140705  \n",
      "min      0.000000  \n",
      "25%      0.000000  \n",
      "50%      0.000000  \n",
      "75%      0.000000  \n",
      "max      1.000000  \n",
      "\n",
      "[8 rows x 26 columns]\n",
      "\n",
      "Size of the dataset before one-hot encoding:\n",
      "(100, 55)\n",
      "\n",
      "Size of the dataset after one-hot encoding:\n",
      "(100, 640)\n"
     ]
    }
   ],
   "source": [
    "# Read data from /Users/grantrobinett/2024/coding/txdot_interpreted_fields/test_improvements/dataLocal/crash_data_1.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Read only the first 100,000 rows, avoiding encoding errors\n",
    "try:\n",
    "    raw_data_1 = pd.read_csv('./dataLocal/crash_data_1.csv', encoding='windows-1252', skiprows=range(1, 100001), nrows=100, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error: Could not read the CSV file with encoding 'windows-1252'\")\n",
    "\n",
    "\n",
    "# List of columns to be predicted and therefore removed from the dataset\n",
    "columns_to_predict = [\n",
    "    \"Road_Relat_ID\", \"Intrsct_Relat_ID\", \"Road_Cls_ID\", \"Harm_Evnt_ID\",\n",
    "    \"FHE_Collsn_ID\", \"Obj_Struck_ID\", \"Phys_Featr_1_ID\", \"Phys_Featr_2_ID\",\n",
    "    \"Bridge_Detail_ID\", \"Othr_Factr_ID\", \"Road_Part_Adj_ID\", \"Investigator_Narrative\"\n",
    "]\n",
    "\n",
    "# Drop the columns to be predicted from the dataset\n",
    "data_features = raw_data_1.drop(columns=columns_to_predict)\n",
    "\n",
    "# Drop columns with any missing values\n",
    "data_features_no_missing = data_features.dropna(axis=1)\n",
    "\n",
    "# Print size of the dataset after dropping columns with missing values\n",
    "print(\"\\nSize of the dataset after dropping columns with missing values:\")\n",
    "print(data_features_no_missing.shape)\n",
    "\n",
    "# Normalize numerical columns in the dataset to be between 0 and 1\n",
    "data_features_normalized = data_features_no_missing.copy()\n",
    "numerical_columns = data_features_normalized.select_dtypes(include=['float64', 'int64']).columns\n",
    "data_features_normalized[numerical_columns] = (data_features_normalized[numerical_columns] - data_features_normalized[numerical_columns].min()) / (data_features_normalized[numerical_columns].max() - data_features_normalized[numerical_columns].min())\n",
    "\n",
    "# Print summary statistics of the normalized dataset\n",
    "print(\"\\nSummary statistics of the normalized dataset:\")\n",
    "print(data_features_normalized.describe())\n",
    "\n",
    "# Encode categorical columns in the dataset using one-hot encoding\n",
    "# Print size of the dataset before one-hot encoding\n",
    "print(\"\\nSize of the dataset before one-hot encoding:\")\n",
    "print(data_features_normalized.shape)\n",
    "data_features_encoded = pd.get_dummies(data_features_normalized)   # One-hot encoding\n",
    "# Print size of the dataset after one-hot encoding\n",
    "print(\"\\nSize of the dataset after one-hot encoding:\")\n",
    "print(data_features_encoded.shape)\n",
    "\n",
    "# Normalize the test data\n",
    "test_features_normalized = data_features_encoded.copy()\n",
    "test_features_normalized[numerical_columns] = (test_features_normalized[numerical_columns] - test_features_normalized[numerical_columns].min()) / (test_features_normalized[numerical_columns].max() - test_features_normalized[numerical_columns].min())\n",
    "\n",
    "# One-hot encode the test data\n",
    "test_features_encoded = pd.get_dummies(test_features_normalized)\n",
    "\n",
    "# Print size of the test data\n",
    "print(\"\\nSize of the test data:\")\n",
    "print(test_features_encoded.shape)\n",
    "\n",
    "# Save pandas dataframe\n",
    "test_features_encoded.to_csv('dataLocal/test_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Length of X_test\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels:\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[[0.932618082, 0.0673819259, 1.19697754e-08, 1.29104189e-18, 3.36653622e-11, 2.6256805e-14, 3.69258306e-18, 6.09361921e-17, 3.08450522e-17], [0.0146303605, 0.0123871742, 0.00430872804, 9.00775485e-05, 0.967940629, 6.49367712e-06, 0.000626048248, 2.56044541e-06, 7.97442863e-06], [0.01033141, 0.978080273, 0.0115832603, 3.47133877e-09, 4.71151452e-06, 3.12500248e-07, 8.09354472e-10, 1.68631803e-10, 1.20626469e-11], [0.70582211, 0.294176251, 1.65792346e-06, 3.35033824e-15, 3.15312287e-09, 1.21390042e-11, 4.49851151e-15, 5.71287605e-14, 5.55513133e-15], [1.44981303e-20, 7.70826283e-24, 9.4596078e-25, 1.99813797e-21, 1.52110058e-14, 4.90598495e-19, 1.20928876e-24, 2.20192708e-31, 1.0], [0.942840159, 0.0571598373, 2.26926899e-09, 7.41073e-21, 1.48105616e-12, 5.59922183e-16, 2.40655565e-20, 7.21464242e-19, 1.68937229e-19], [0.0133731468, 0.011689947, 0.00236482709, 3.4065929e-06, 0.972421, 5.42751764e-07, 0.000146282298, 1.75549218e-07, 7.27250438e-07], [0.00479474524, 0.00277146162, 0.000679927936, 4.18829472e-07, 0.991722763, 2.30344153e-08, 3.05635112e-05, 9.33025923e-09, 5.85069202e-08], [0.00577099621, 0.00353274844, 0.000936880067, 1.17694844e-06, 0.989706397, 5.681337e-08, 5.16006367e-05, 2.72483334e-08, 1.11726578e-07], [0.0324459262, 0.0415015258, 0.0131665962, 0.00090227148, 0.909199, 0.000132401, 0.00253436156, 4.16520525e-05, 7.62382188e-05], [0.0022007497, 0.1364021, 0.861357212, 1.11513127e-05, 2.11967599e-05, 7.59877867e-06, 3.21501159e-09, 2.17775895e-10, 2.25206284e-10], [0.0330097489, 0.964725435, 0.00225814874, 1.90415839e-09, 6.48211426e-06, 2.19539459e-07, 1.26939159e-09, 5.6836047e-10, 3.4453787e-11], [0.0269971546, 0.942899704, 0.0294965301, 4.82485893e-06, 0.000532981416, 6.67832646e-05, 1.66603729e-06, 3.72447886e-07, 6.93605742e-08], [0.0239965394, 0.722213268, 0.251825184, 0.000141816796, 0.00124221877, 0.000575775746, 3.87024147e-06, 8.05174636e-07, 4.47212216e-07], [2.23793849e-27, 7.08465156e-32, 5.64826578e-33, 5.1704725e-29, 2.33232259e-19, 1.02328368e-25, 9.30070159e-33, 0.0, 1.0], [0.024352245, 0.923481524, 0.0512438416, 1.08626646e-05, 0.000794696913, 0.000112883899, 3.21085031e-06, 5.38103791e-07, 1.352075e-07], [0.0393769369, 0.0590030663, 0.017418256, 0.00146603468, 0.87894088, 0.000272138685, 0.00332828634, 8.18759945e-05, 0.000112528389], [0.895520449, 0.104479454, 5.87841456e-08, 3.32294665e-17, 1.96886257e-10, 3.90872474e-13, 6.18772457e-17, 9.42943653e-16, 3.79643158e-16], [0.000806751894, 0.0957000554, 0.903489649, 7.63910862e-07, 2.26491579e-06, 5.37949234e-07, 7.33034883e-11, 2.72837699e-12, 2.57043128e-12], [0.00580853969, 0.00212548813, 0.0105835553, 0.630374312, 0.350858539, 9.98934847e-05, 0.000134121452, 2.69704128e-06, 1.28419069e-05], [0.0221187435, 0.970424354, 0.00744097, 1.56419606e-08, 1.51642598e-05, 8.3754378e-07, 4.7342894e-09, 1.63758762e-09, 1.19686677e-10], [4.80672129e-30, 5.04841e-35, 3.21255202e-36, 8.28184847e-32, 2.87126629e-21, 3.17702752e-28, 5.6484516e-36, 0.0, 1.0], [0.0166509356, 0.87444669, 0.108359061, 8.26946234e-06, 0.000491134648, 4.29885404e-05, 9.29337318e-07, 9.44655341e-08, 2.43370426e-08], [0.0671603531, 0.571796358, 0.125801489, 0.0665688589, 0.108171731, 0.0548209846, 0.00341658643, 0.00110531412, 0.0011583094], [0.012995231, 0.0105130766, 0.0038347072, 9.51473849e-05, 0.971994281, 5.57265912e-06, 0.00055288861, 2.04187927e-06, 7.04313061e-06], [0.0592855923, 0.780951679, 0.0878457129, 0.0076106023, 0.0511991046, 0.0108164903, 0.00171119126, 0.000375829346, 0.000203851945], [0.959188, 0.0408120714, 1.3731315e-09, 8.49992541e-21, 1.91791873e-12, 4.84110934e-16, 3.64220161e-20, 8.39334532e-19, 4.04356567e-19], [0.961495757, 0.0385042727, 1.16530352e-09, 5.12129853e-21, 1.45720372e-12, 3.1956912e-16, 2.52128189e-20, 5.88867293e-19, 2.78898652e-19], [1.47848129e-28, 2.92492125e-33, 2.1740054e-34, 3.01251649e-30, 3.56234023e-20, 7.36652488e-27, 3.71961333e-34, 0.0, 1.0], [0.0192488879, 0.0191495884, 0.00586385699, 0.000113974034, 0.954809546, 1.12352163e-05, 0.000788331, 4.71969679e-06, 9.78122171e-06], [0.929422438, 0.0705775395, 1.00548192e-08, 3.20407557e-19, 1.58079591e-11, 8.56459831e-15, 1.06180491e-18, 2.0580826e-17, 6.25354333e-18], [0.00917139277, 0.00670671882, 0.0017131744, 3.39317739e-06, 0.982293904, 2.63604051e-07, 0.000110775167, 1.09547727e-07, 3.84949914e-07], [0.0110180061, 0.00863359496, 0.00233873515, 8.08483674e-06, 0.977816463, 6.18799561e-07, 0.000183541066, 2.98044824e-07, 8.04358081e-07], [0.00621103449, 0.22598885, 0.767469, 9.1313188e-05, 0.000168290542, 7.13848785e-05, 1.11496597e-07, 1.19659518e-08, 1.17733157e-08], [0.0382725596, 0.0590206459, 0.0186324678, 0.00373721262, 0.875215054, 0.000537913584, 0.00425650692, 0.000145254831, 0.000182492397], [0.0860941857, 0.626410902, 0.065001078, 0.00440390734, 0.212330088, 0.0030068364, 0.00233123288, 0.000335885939, 8.58638741e-05], [0.0520365871, 0.825794041, 0.0804916844, 0.00307445461, 0.0324744023, 0.00506838923, 0.000817994645, 0.000164370533, 7.81037452e-05], [0.00365377939, 0.187491357, 0.808762789, 2.29131274e-05, 5.19925452e-05, 1.72070759e-05, 1.49663197e-08, 1.18338195e-09, 1.04052933e-09], [0.910443604, 0.089556329, 5.4303527e-08, 6.42512673e-17, 3.88765825e-10, 4.66162482e-13, 1.58056361e-16, 1.92367027e-15, 1.0820451e-15], [0.0205672067, 0.0230001491, 0.00424494036, 1.52734738e-05, 0.951857746, 2.37336963e-06, 0.000309886644, 1.23340453e-06, 1.18591379e-06], [0.959864616, 0.0401354283, 1.25621591e-09, 7.07995599e-21, 1.60295106e-12, 4.77688201e-16, 2.92916966e-20, 7.45352546e-19, 3.54877518e-19], [0.0170229357, 0.0151269455, 0.00525874458, 0.000110536923, 0.961688638, 9.67255619e-06, 0.00076774531, 3.50501546e-06, 1.12612797e-05], [0.107217714, 0.892430544, 0.000350555812, 1.23057217e-10, 1.1920215e-06, 4.22185309e-08, 6.72922e-11, 1.39927819e-10, 8.71784242e-12], [0.00847486593, 0.00657587731, 0.00114012975, 4.70771766e-07, 0.983769119, 4.47863364e-08, 3.94232411e-05, 2.2699945e-08, 4.89149734e-08], [0.00892253686, 0.0074539436, 0.00121507759, 8.53166284e-07, 0.982364178, 7.29238039e-08, 4.32588458e-05, 4.45583872e-08, 5.02698612e-08], [0.700694323, 0.299295485, 1.00567713e-05, 3.42169956e-12, 1.91273244e-07, 2.22498309e-09, 3.49289543e-12, 2.0265253e-11, 7.79021379e-12], [0.0876220688, 0.612433493, 0.0788297132, 0.0103011914, 0.19833298, 0.00760826189, 0.00389303616, 0.000712064095, 0.000267228112], [0.0462067425, 0.0838177949, 0.016119035, 0.000512982078, 0.851212382, 0.000139462354, 0.0019103525, 5.20179819e-05, 2.91399265e-05], [0.00193878589, 0.249622688, 0.748432159, 5.08201651e-07, 5.21453603e-06, 6.1256344e-07, 2.11495751e-10, 1.0382791e-11, 3.9598355e-12], [0.0010034499, 0.116018787, 0.882973373, 8.56609631e-07, 2.96671851e-06, 6.36091613e-07, 1.04647763e-10, 4.37268711e-12, 3.46135685e-12], [0.024111025, 0.0251127835, 0.00956003554, 0.000850742857, 0.937933922, 8.67562849e-05, 0.00223769573, 2.63634556e-05, 8.07146498e-05], [0.838141143, 0.161858425, 3.94509215e-07, 1.96936283e-15, 2.60921795e-09, 7.27945308e-12, 3.3271505e-15, 3.46570137e-14, 1.18274497e-14], [0.0101936255, 0.00797682, 0.00181076757, 2.81913935e-06, 0.979907036, 2.61363937e-07, 0.000108201421, 1.17114809e-07, 3.23091854e-07], [0.0220902488, 0.0202307496, 0.00821821112, 0.000534416875, 0.946756482, 5.62876594e-05, 0.00201704516, 1.63594104e-05, 8.01203132e-05], [4.51908064e-30, 4.63968539e-35, 2.67319246e-36, 6.64714314e-32, 2.47039469e-21, 2.93253721e-28, 4.65019079e-36, 0.0, 1.0], [0.00798309874, 0.946816206, 0.0451961234, 9.97556526e-09, 4.23293113e-06, 3.2948617e-07, 3.44410334e-10, 6.06966769e-11, 5.16065e-12], [1.23208864e-28, 2.17519279e-33, 1.22786228e-34, 2.53849679e-30, 2.38208145e-20, 7.4277e-27, 2.06178036e-34, 0.0, 1.0], [0.0186088439, 0.0160611141, 0.00574463746, 0.000119382588, 0.958363652, 1.78084665e-05, 0.00104560261, 3.70738076e-06, 3.52472816e-05], [0.029181974, 0.0360416844, 0.00942371, 0.000229993151, 0.923759699, 3.69762056e-05, 0.00129330577, 1.36198678e-05, 1.9108762e-05], [0.987336576, 0.0126634287, 5.78280791e-11, 1.85190651e-22, 2.32263563e-13, 1.99537106e-17, 1.4393705e-21, 3.34884782e-20, 6.71554335e-20], [0.0284579732, 0.036684975, 0.00609090179, 3.35454533e-05, 0.928150952, 8.27359054e-06, 0.000565625494, 3.6031654e-06, 4.04587308e-06], [0.0234620944, 0.585577071, 0.387647718, 0.000428844913, 0.00194522412, 0.000928872963, 7.78167487e-06, 1.46964214e-06, 1.01195133e-06], [0.954403043, 0.045596987, 4.57224969e-09, 4.83501182e-19, 2.12104587e-11, 1.04512499e-14, 1.79172325e-18, 2.71604e-17, 2.58342137e-17], [0.965867758, 0.0341322571, 4.35835618e-10, 3.08623035e-22, 2.60589158e-13, 4.27857782e-17, 1.58071872e-21, 4.85377079e-20, 1.7605032e-20], [0.0078502018, 0.980139375, 0.012004979, 2.51151788e-09, 5.27146085e-06, 1.93797803e-07, 9.17602772e-10, 1.13374921e-10, 7.23085957e-12], [0.00890274253, 0.448035151, 0.542889357, 1.63812947e-05, 0.000122874713, 3.33914941e-05, 5.63094105e-08, 7.13097359e-09, 3.33921246e-09], [0.0132063311, 0.010433156, 0.00298885535, 1.69199648e-05, 0.973042905, 1.58397006e-06, 0.000307396462, 5.93918628e-07, 2.23077132e-06], [0.0228666347, 0.67476815, 0.297900856, 0.000542099879, 0.00307760527, 0.000828717835, 1.33734311e-05, 1.6172047e-06, 1.10115923e-06], [7.09676675e-25, 5.9112652e-29, 3.80509891e-30, 5.84305205e-26, 1.01981187e-17, 3.98255513e-23, 5.45929218e-30, 6.75040895e-38, 1.0], [0.0148410946, 0.0244470686, 0.0348580368, 0.788728058, 0.125155717, 0.0107488595, 0.000529186218, 0.000123728649, 0.000568283489], [1.06647296e-17, 2.37561118e-20, 6.33219401e-21, 2.35785518e-18, 3.14355052e-12, 1.53159134e-16, 9.01862824e-21, 3.31663125e-27, 1.0], [0.00468292274, 0.00155633886, 0.00862085633, 0.693016887, 0.291949332, 8.22275688e-05, 8.02431459e-05, 1.55780276e-06, 9.68126096e-06], [0.0078244172, 0.232417375, 0.759010255, 0.000228331381, 0.000313936791, 0.000205203978, 3.21273234e-07, 4.42075461e-08, 5.34818447e-08], [0.0375410654, 0.873363197, 0.0721305162, 0.000733829162, 0.0141658187, 0.0017966287, 0.000221596172, 3.2480104e-05, 1.48718491e-05], [2.65450832e-26, 1.39277611e-30, 1.34597018e-31, 1.13029237e-27, 1.50206342e-18, 1.34594393e-24, 2.06114659e-31, 0.0, 1.0], [0.0551753677, 0.505136788, 0.207551733, 0.0666715801, 0.0612988509, 0.100278609, 0.0016722423, 0.000747136, 0.00146768], [0.00958528928, 0.00746177509, 0.00158511288, 1.59709168e-06, 0.981287956, 1.52955494e-07, 7.78348622e-05, 6.60056e-08, 1.85908988e-07], [0.00359635404, 0.000742653734, 0.00287206238, 0.148390383, 0.844125628, 9.73600436e-06, 0.000253320381, 1.06197331e-06, 8.82227687e-06], [0.945714116, 0.0542858616, 6.24935659e-09, 4.36774421e-19, 1.78861908e-11, 1.06370126e-14, 1.31006032e-18, 2.32367028e-17, 1.50474493e-17], [0.0148988971, 0.0143770976, 0.00273241568, 6.61574677e-06, 0.967818201, 7.26368e-07, 0.000165129546, 4.2186349e-07, 4.64743835e-07], [0.0245307181, 0.0275941789, 0.00688142469, 8.14435916e-05, 0.940065444, 1.266659e-05, 0.000819720211, 5.3624417e-06, 9.02016563e-06], [0.0611520968, 0.937980771, 0.000866342394, 2.30780908e-10, 6.72410749e-07, 1.48121089e-07, 3.76702905e-11, 7.19023105e-11, 7.12861367e-12], [0.0173494872, 0.0183723606, 0.00322746718, 8.07009928e-06, 0.960837841, 1.07908988e-06, 0.000202507363, 6.55911492e-07, 5.61984e-07], [0.0152194491, 0.0149849011, 0.00289228139, 5.87101431e-06, 0.966731071, 7.43120097e-07, 0.000164831159, 3.84845293e-07, 5.56605187e-07], [0.959275246, 0.0407246873, 2.69346878e-09, 2.11841588e-19, 1.31217728e-11, 5.81118134e-15, 7.42489105e-19, 1.37102366e-17, 1.12995213e-17], [2.10429285e-32, 8.06569347e-38, 0.0, 1.93869669e-34, 5.68233234e-23, 1.65113849e-30, 0.0, 0.0, 1.0], [0.987668097, 0.0123319058, 2.518513e-11, 4.73205511e-24, 2.33614109e-14, 1.37825092e-18, 4.11851846e-23, 1.36973678e-21, 1.50219968e-21], [0.011598995, 0.01174815, 0.0257047173, 0.774041057, 0.173841521, 0.002480336, 0.000363915169, 4.05037536e-05, 0.000180853574], [0.0344219245, 0.619676828, 0.33170253, 0.00262500276, 0.00769093726, 0.00378303719, 7.16949435e-05, 1.56063652e-05, 1.24818571e-05], [0.923219502, 0.0767804906, 2.09685247e-08, 3.67044688e-18, 6.59906435e-11, 5.04955744e-14, 1.0517417e-17, 1.49783549e-16, 7.44208375e-17], [0.0097699808, 0.00553667, 0.0166883804, 0.620662, 0.34623611, 0.000570892706, 0.000438981107, 2.02176197e-05, 7.67972087e-05], [0.00219750567, 0.204485402, 0.793303311, 1.82417693e-06, 1.0012337e-05, 2.01301805e-06, 8.01579914e-10, 4.51018806e-11, 2.67573238e-11], [0.968013227, 0.0319867656, 9.87575577e-10, 2.6676809e-20, 3.84024496e-12, 1.18254259e-15, 1.09908276e-19, 2.35498589e-18, 1.85969516e-18], [0.0520012937, 0.116820022, 0.0160607416, 0.000466623344, 0.812821388, 0.000149856161, 0.00160057144, 6.36776313e-05, 1.57847699e-05], [0.0777334347, 0.248541072, 0.0389209278, 0.00466724951, 0.622464657, 0.00189668615, 0.00506864348, 0.00056006026, 0.000147198953], [0.0223079212, 0.0300651714, 0.00492197787, 2.94681704e-05, 0.942350388, 4.46766808e-06, 0.000317321304, 2.15737168e-06, 1.19613173e-06], [9.85121231e-29, 1.71520109e-33, 1.04868491e-34, 2.61779865e-30, 2.16297746e-20, 6.91873153e-27, 1.69154188e-34, 0.0, 1.0], [0.00169673702, 0.155624092, 0.842668116, 1.83243981e-06, 7.60521834e-06, 1.6544094e-06, 5.43076917e-10, 2.73349121e-11, 2.02703878e-11], [0.997716546, 0.00228347443, 7.07258349e-14, 7.97065848e-29, 3.59951085e-17, 2.54064532e-22, 1.48720272e-27, 9.51039367e-26, 2.98297176e-25], [0.0275439043, 0.0379805602, 0.00492203748, 1.70330568e-05, 0.92920357, 3.55224779e-06, 0.000326010224, 2.3626319e-06, 9.27258554e-07]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Print class labels\n",
    "print(\"Class labels:\")\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Get first 10 rows of X_test\n",
    "X_test_10 = X_test.head(100)\n",
    "\n",
    "# Convert DataFrame to a list of lists for TensorFlow Serving\n",
    "data = X_test_10.values.tolist()\n",
    "\n",
    "# Save the data to a file\n",
    "with open('dataLocal/request_data_raw.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "# Format the POST request data\n",
    "request_data = json.dumps({\"instances\": data})\n",
    "\n",
    "# Export the request data to a file\n",
    "with open('dataLocal/request_data.json', 'w') as f:\n",
    "    f.write(request_data)\n",
    "\n",
    "# Specify the TensorFlow Serving REST API endpoint\n",
    "tf_serving_url = 'http://localhost:8501/v1/models/model:predict'  # Adjust the model name if necessary\n",
    "\n",
    "# # Specify the TensorFlow Serving REST API endpoint\n",
    "# tf_serving_url = \"https://test-inference.herokuapp.com/v1/models/ANN_v2:predict\"  # Adjust the model name if necessary\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(tf_serving_url, data=request_data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    # Load the JSON response\n",
    "    predictions = response.json()['predictions']\n",
    "    # Do something with the predictions\n",
    "    print(predictions)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 1 0 8 0 4 4 4 4 2 1 1 1 8 1 4 0 2 3 1 8 1 1 4 1 0 0 8 4 0 4 4 2 4 1 1\n",
      " 2 0 4 0 4 1 4 4 0 1 4 2 2 4 0 4 4 8 1 8 4 4 0 4 1 0 0 1 2 4 1 8 3 8 3 2 1\n",
      " 8 1 4 4 0 4 4 1 4 4 0 8 0 3 1 0 3 2 0 4 4 4 8 2 0 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the predictions into a numpy array for easier manipulation\n",
    "predictions_array = np.array(predictions)\n",
    "\n",
    "# Get the index of the max probability for each prediction\n",
    "predicted_classes = np.argmax(predictions_array, axis=1)\n",
    "\n",
    "# Do something with the predicted classes\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        22\n",
      "           1       0.81      0.77      0.79        22\n",
      "           2       0.70      0.78      0.74         9\n",
      "           3       0.80      0.80      0.80         5\n",
      "           4       0.88      0.94      0.91        31\n",
      "           8       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.88      0.88      0.88       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Check first 10 values of y_test\n",
    "# print(y_test[:100])\n",
    "# Decode the one-hot encoded labels\n",
    "\n",
    "# # Print y classes\n",
    "# print(label_encoder.classes_)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Decode the one-hot encoded labels\n",
    "decoded_labels = label_encoder.inverse_transform(np.argmax(y_test[:100], axis=1))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(decoded_labels, predicted_classes)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(decoded_labels, predicted_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from /Users/grantrobinett/2024/coding/txdot_interpreted_fields/test_improvements/dataLocal/crash_data_1.csv and save 100,000 rows to a new file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the first 100,000 rows of the dataset\n",
    "try:\n",
    "    raw_data_1 = pd.read_csv('./dataLocal/crash_data_1.csv', encoding='windows-1252', nrows=100000, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    print(\"Error: Could not read the CSV file with encoding 'windows-1252'\")\n",
    "    \n",
    "# Save the first 100,000 rows to a new file\n",
    "raw_data_1.to_csv('dataLocal/crash_data_100k.csv', index=False)\n",
    "\n",
    "# Also save it as a XML file\n",
    "raw_data_1.to_xml('dataLocal/crash_data_100k.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Crash_ID</th>\n",
       "      <th>Crash_Fatal_Fl</th>\n",
       "      <th>Cmv_Involv_Fl</th>\n",
       "      <th>Schl_Bus_Fl</th>\n",
       "      <th>Rr_Relat_Fl</th>\n",
       "      <th>Medical_Advisory_Fl</th>\n",
       "      <th>Amend_Supp_Fl</th>\n",
       "      <th>Active_School_Zone_Fl</th>\n",
       "      <th>Crash_Date</th>\n",
       "      <th>...</th>\n",
       "      <th>Unkn_Injry_Cnt</th>\n",
       "      <th>Tot_Injry_Cnt</th>\n",
       "      <th>Death_Cnt</th>\n",
       "      <th>MPO_ID</th>\n",
       "      <th>Investigat_Service_ID</th>\n",
       "      <th>Investigat_DA_ID</th>\n",
       "      <th>Damage_1</th>\n",
       "      <th>Damage_2</th>\n",
       "      <th>Damage_3</th>\n",
       "      <th>Investigator_Narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18696335</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>01/07/2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>None</td>\n",
       "      <td>POWER POLE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>UNIT 1 TRAVELING SOUTHBOUND ON FM 740 ATTEMPTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18688350</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>01/06/2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>UNIT 1 WAS TURNING NB ONTO 1200 8TH AVE FROM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18688437</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>01/09/2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>21.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>UNIT 2 UNIT 3 UNIT 4 AND UNIT 5 WERE PARKED AT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18688481</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>01/07/2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>UNIT 2 DRIVER STATED THAT HE WAS TURNING INTO ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18688480</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>01/10/2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>UNIT 2 WAS IN THE LEFT LANE TRAVELING SOUTH ON...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  index  Crash_ID Crash_Fatal_Fl Cmv_Involv_Fl Schl_Bus_Fl Rr_Relat_Fl  \\\n",
       "0     0  18696335              N             N           N           N   \n",
       "1     1  18688350              N             N           N           N   \n",
       "2     2  18688437              N             N           N           N   \n",
       "3     3  18688481              N             N           N           N   \n",
       "4     4  18688480              N             N           N           N   \n",
       "\n",
       "  Medical_Advisory_Fl Amend_Supp_Fl Active_School_Zone_Fl  Crash_Date  ...  \\\n",
       "0                   N             N                     N  01/07/2022  ...   \n",
       "1                   N             N                     N  01/06/2022  ...   \n",
       "2                   N             N                     N  01/09/2022  ...   \n",
       "3                   N             N                     N  01/07/2022  ...   \n",
       "4                   N             N                     N  01/10/2022  ...   \n",
       "\n",
       "  Unkn_Injry_Cnt Tot_Injry_Cnt Death_Cnt MPO_ID Investigat_Service_ID  \\\n",
       "0              0             0         0  282.0                  45.0   \n",
       "1              0             0         0  282.0                  21.0   \n",
       "2              0             0         0   None                  21.0   \n",
       "3              0             0         0   28.0                 175.0   \n",
       "4              0             0         0   90.0                 118.0   \n",
       "\n",
       "  Investigat_DA_ID    Damage_1 Damage_2 Damage_3  \\\n",
       "0             None  POWER POLE     None     None   \n",
       "1              4.0        None     None     None   \n",
       "2             None        None     None     None   \n",
       "3             None        None     None     None   \n",
       "4             None        None     None     None   \n",
       "\n",
       "                              Investigator_Narrative  \n",
       "0  UNIT 1 TRAVELING SOUTHBOUND ON FM 740 ATTEMPTE...  \n",
       "1   UNIT 1 WAS TURNING NB ONTO 1200 8TH AVE FROM ...  \n",
       "2  UNIT 2 UNIT 3 UNIT 4 AND UNIT 5 WERE PARKED AT...  \n",
       "3  UNIT 2 DRIVER STATED THAT HE WAS TURNING INTO ...  \n",
       "4  UNIT 2 WAS IN THE LEFT LANE TRAVELING SOUTH ON...  \n",
       "\n",
       "[5 rows x 175 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read new XML file and load first 1000 rows to a pandas DataFrame\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('dataLocal/crash_data_100k.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each row in the XML file\n",
    "for row in root:\n",
    "    # Define an empty dictionary to store the row data\n",
    "    row_data = {}\n",
    "    # Loop through each column in the row\n",
    "    for col in row:\n",
    "        # Add the column name and value to the dictionary\n",
    "        row_data[col.tag] = col.text\n",
    "    # Append the row data to the list\n",
    "    data.append(row_data)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Load the first 1000 rows of the DataFrame\n",
    "df_1000 = df.head(1000)\n",
    "\n",
    "df_1000.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the first 1000 rows of the DataFrame to a json file\n",
    "df_1000.to_json('dataLocal/crash_data_1000.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 400\n",
      "Response: {'error': '{\\n    \"error\": \"Matrix size-incompatible: In[0]: [1000,6056], In[1]: [4981,256]\\\\n\\\\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential_2/dense_10/Relu}}]]\"\\n}'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:5000/predict'\n",
    "# Load data from test_improvements/dataLocal/crash_data_1000.json into json\n",
    "data = df_1000.to_json(orient='records')\n",
    "data = json.loads(data)\n",
    "\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "print('Status Code:', response.status_code)\n",
    "print('Response:', response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
